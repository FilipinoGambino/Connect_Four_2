#defaults:
#- override hydra/job_logging: colorlog
#- override hydra/hydra_logging: colorlog

hydra:
  run:
    dir: ./outputs/${now:%m-%d}/${now:%H-%M-%S}

name: conv_phase2
## WANDB params
# The wandb project name
project: ConnectX
# The wandb user to log to
entity: filipinogambino
# The wandb group for the run
group: diagonals

# Parameters to overwrite
use_mixed_precision: false
total_steps: 5e5
batch_size: 8
checkpoint_freq: 60.
num_actors: 2
n_actor_envs: 16
unroll_length: 16
seed: 42
model_arch: conv_model
embedding_dim: 32
hidden_dim: 128
kernel_size: 3
n_blocks: 12
device: cpu
rescale_value_input: false
rescale_se_input: false
obs_space_kwargs: {}
reward_space_kwargs: {}
normalize: false
debug: false

# Model params
act_space: BasicActionSpace
obs_space: HistoricalObs
reward_space: DiagonalEmphasisReward

## OPTIMIZER params
optimizer_class: Adam
optimizer_kwargs:
  lr: 5e-5
  # See https://arxiv.org/pdf/2105.05246.pdf
  eps: 0.0003
  # alpha: 0.9
min_lr_mod: 0.02

## LOSS params
entropy_cost: 0.001
baseline_cost: 1.
teacher_kl_cost: 0.005
# lambda parameter for TD-lambda and UPGO losses
lmb: 0.8
reduction: mean

# Pretrained model for KL loss
use_teacher: true
teacher_load_dir: outputs/03-13/21-43-05
teacher_checkpoint_file: 1000448_weights.pt

# MISCELLANEOUS params
actor_device: cpu
learner_device: cpu
disable_wandb: false
model_log_freq: 100
# file_descriptor or file_system
sharing_strategy: file_descriptor